{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1A2R2Lk42hZ1tnaIRzLhTFyMjdelsLtEr",
      "authorship_tag": "ABX9TyOlgYnohVcQQj3D3MlmU+F1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ai-phasepro/LRECA/blob/main/Demo/code_for_model_testing/LRECA_test_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LRECA test Demo**\n",
        "\n",
        "## **Preparation**\n",
        "\n",
        "**Before you start using the demos, please run code cell 0.1 and code cell 0.2 first, which you only need to run once if they run with no error.**\n",
        "\n",
        "## **Test Data Input**\n",
        "\n",
        "Then choose to run **code cell 1.1 or code cell 1.2** according to your test data, If you test **a single sequence**, please replace the sequence **‘MSGGG...RSRT’** in the code of code cell 1.1 with your sequence, and then run **code cell 1.1**.\n",
        "\n",
        "if you test **multiple sequences**, please **upload the txt file** of one sequence per line to colab and **modify the path(pos_test_dir & neg_test_dir)** of the test file to **run code cell 1.2**.\n",
        "\n",
        "## **Define the model and run the tests**\n",
        "\n",
        "Then, run **code cell 2** to define the model (which is usually only run once when testing multiple times in a runtime),\n",
        "\n",
        "Finally, run **code cell 3** to **run the model** and output the test results.\n",
        "\n",
        "## **output**\n",
        "\n",
        "For **single** sequences the test results are displayed in the the last line  of the code cell 3.The output of the example is “'y_score': 0.9564854502677917, 'y_pre': 1”, in which ‘y_score’ is the predicted value of the input sequence and ‘y_pre’ is the predicted label (1 represents LLPS positive, and 0 represents LLPS negative).\n",
        "\n",
        "For test results for **multiple** sequences,\n",
        "\n",
        "the **predicted results for each sequence** are output in **classification_output/personal_output/personal_test_roc_1.csv **\n",
        "\n",
        "the **test metrics** are output in\n",
        "**classification_output/personal_output/result_1.csv**\n",
        "Note: the output test metrics in result_1.csv are meaningful only when the sequences in the upload files under the ‘pos_test_dir’ and ‘neg_test_dir’ paths have been determined to be LLPS positive and negative, respectively. Otherwise, the result_1.csv has no reference value.\n",
        "\n",
        "\n",
        "## **help**\n",
        "\n",
        "### **How to upload a test fileset to colab**\n",
        "\n",
        "Here are two ways to upload a test dataset file.\n",
        "\n",
        "1. Click on the **\"File\" icon** on **the left sidebar**, and then click on \"**Upload to session storge**\" at the top of the pop-up file sidebar to upload a file.\n",
        "\n",
        "2. Run **code cell 0.3** and select file upload\n",
        "\n",
        "See the official [Google Colab instructions](https://colab.research.google.com/notebooks/io.ipynb#scrollTo=7Z2jcRKwUHqV) for more file uploading options\n",
        "\n",
        "### **When testing multiple sequences using the txt file format, do I have to split the positive and negative data to upload?**\n",
        "\n",
        "**Not required**, distinguishing between positives and negatives for test set files is a performance metric that facilitates statistical modeling. If you are testing an unlabeled dataset or just want to see the predictions of the sequences, **you can set only one of the paths (pos_test_dir / neg_test_dir) to the path of the file to be tested and set the other to an empty string (\"\")**, in **which case the performance metrics are meaningless**. In this case, the performance metrics are meaningless. You can see the results for each sequence in the sequence prediction output file (classification_output/personal_output/personal_test_roc_1.csv).\n",
        "\n"
      ],
      "metadata": {
        "id": "OhNYlFAvyulA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 0.1 First run the code grid to download the data needed for the demo and unzip it.\n",
        "!gdown \"https://drive.google.com/uc?export=download&id=1AfIylZdDgd7yZYD2seVw9998SpBVM0BW\"\n",
        "!unzip LRECA_testdata.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0hpbOEKiX1bp",
        "outputId": "06c8f5cc-e324-46a5-cd2e-f77b87994397"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?export=download&id=1AfIylZdDgd7yZYD2seVw9998SpBVM0BW\n",
            "To: /content/LRECA_testdata.zip\n",
            "\r  0% 0.00/4.92M [00:00<?, ?B/s]\r100% 4.92M/4.92M [00:00<00:00, 106MB/s]\n",
            "Archive:  LRECA_testdata.zip\n",
            "  inflating: model_mydata_1.pt       \n",
            "  inflating: neg_word_list_1479.txt  \n",
            "  inflating: neg_word_list_mydata_test.txt  \n",
            "  inflating: pos_word_list_mydata_all_1507.txt  \n",
            "  inflating: pos_word_list_mydata_test.txt  \n",
            "  inflating: requirements.txt        \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 0.2 Configure the runtime environment\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu # if using GPU runtime,please comment out this line\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "h8ugGI1pM00k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4388d043-b11a-46b9-e947-fffe1def691a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cpu\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.1+cpu)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.18.1+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.3.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: asttokens==2.4.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (2.4.1)\n",
            "Requirement already satisfied: backcall==0.2.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (0.2.0)\n",
            "Requirement already satisfied: bio==1.6.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (1.6.0)\n",
            "Requirement already satisfied: biopython==1.81 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (1.81)\n",
            "Requirement already satisfied: biothings-client==0.3.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (0.3.1)\n",
            "Requirement already satisfied: charset-normalizer==2.1.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (2.1.1)\n",
            "Requirement already satisfied: comm==0.2.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (0.2.0)\n",
            "Requirement already satisfied: contourpy==1.1.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (1.1.1)\n",
            "Requirement already satisfied: cycler==0.12.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (0.12.1)\n",
            "Requirement already satisfied: et-xmlfile==1.1.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (1.1.0)\n",
            "Requirement already satisfied: executing==2.0.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (2.0.1)\n",
            "Requirement already satisfied: filelock==3.9.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 12)) (3.9.0)\n",
            "Requirement already satisfied: fonttools==4.45.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 13)) (4.45.0)\n",
            "Requirement already satisfied: gensim==4.3.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 14)) (4.3.2)\n",
            "Requirement already satisfied: gprofiler-official==1.0.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 15)) (1.0.0)\n",
            "Requirement already satisfied: idna==3.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 16)) (3.4)\n",
            "Requirement already satisfied: importlib-metadata==6.9.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 17)) (6.9.0)\n",
            "Requirement already satisfied: importlib-resources==6.1.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 18)) (6.1.1)\n",
            "Requirement already satisfied: jedi==0.19.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 19)) (0.19.1)\n",
            "Requirement already satisfied: Jinja2==3.1.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 20)) (3.1.2)\n",
            "Requirement already satisfied: joblib==1.3.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 21)) (1.3.2)\n",
            "Requirement already satisfied: jupyter_core==5.5.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 22)) (5.5.0)\n",
            "Requirement already satisfied: kiwisolver==1.4.5 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 23)) (1.4.5)\n",
            "Requirement already satisfied: MarkupSafe==2.1.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 24)) (2.1.3)\n",
            "Requirement already satisfied: mpmath==1.3.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 25)) (1.3.0)\n",
            "Requirement already satisfied: mygene==3.2.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 26)) (3.2.2)\n",
            "Requirement already satisfied: nest-asyncio==1.5.8 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 27)) (1.5.8)\n",
            "Requirement already satisfied: networkx==3.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 28)) (3.0)\n",
            "Requirement already satisfied: openpyxl==3.1.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 29)) (3.1.2)\n",
            "Requirement already satisfied: packaging==23.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 30)) (23.2)\n",
            "Requirement already satisfied: pandas==2.0.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 31)) (2.0.3)\n",
            "Requirement already satisfied: parso==0.8.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 32)) (0.8.3)\n",
            "Requirement already satisfied: pexpect==4.9.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 33)) (4.9.0)\n",
            "Requirement already satisfied: pickle5==0.0.11 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 34)) (0.0.11)\n",
            "Requirement already satisfied: pickleshare==0.7.5 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 35)) (0.7.5)\n",
            "Requirement already satisfied: platformdirs==4.0.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 36)) (4.0.0)\n",
            "Requirement already satisfied: pooch==1.8.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 37)) (1.8.0)\n",
            "Requirement already satisfied: ptyprocess==0.7.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 38)) (0.7.0)\n",
            "Requirement already satisfied: pure-eval==0.2.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 39)) (0.2.2)\n",
            "Requirement already satisfied: Pygments==2.17.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 40)) (2.17.2)\n",
            "Requirement already satisfied: pyparsing==3.1.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 41)) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil==2.8.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 42)) (2.8.2)\n",
            "Requirement already satisfied: pytz==2023.3.post1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 43)) (2023.3.post1)\n",
            "Requirement already satisfied: scikit-learn==1.3.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 44)) (1.3.2)\n",
            "Requirement already satisfied: scipy==1.10.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 45)) (1.10.1)\n",
            "Requirement already satisfied: seaborn==0.13.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 46)) (0.13.0)\n",
            "Requirement already satisfied: SimpleITK==2.3.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 47)) (2.3.0)\n",
            "Requirement already satisfied: six==1.16.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 48)) (1.16.0)\n",
            "Requirement already satisfied: smart-open==6.4.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 49)) (6.4.0)\n",
            "Requirement already satisfied: stack-data==0.6.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 50)) (0.6.3)\n",
            "Requirement already satisfied: sympy==1.12 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 51)) (1.12)\n",
            "Requirement already satisfied: threadpoolctl==3.2.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 52)) (3.2.0)\n",
            "Requirement already satisfied: tqdm==4.66.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 53)) (4.66.1)\n",
            "Requirement already satisfied: traitlets==5.14.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 54)) (5.14.0)\n",
            "Requirement already satisfied: tzdata==2023.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 55)) (2023.3)\n",
            "Requirement already satisfied: urllib3==1.26.13 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 56)) (1.26.13)\n",
            "Requirement already satisfied: xlrd==2.0.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 57)) (2.0.1)\n",
            "Requirement already satisfied: zipp==3.17.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 58)) (3.17.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bio==1.6.0->-r requirements.txt (line 3)) (2.31.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from biopython==1.81->-r requirements.txt (line 4)) (1.25.2)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.3 in /usr/local/lib/python3.10/dist-packages (from seaborn==0.13.0->-r requirements.txt (line 46)) (3.7.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.3->seaborn==0.13.0->-r requirements.txt (line 46)) (9.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bio==1.6.0->-r requirements.txt (line 3)) (2024.7.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 0.3 upload file to colab\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "    print('User uploaded file \"{name}\" with length {length} bytes'.format(name=fn, length=len(uploaded[fn])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 40
        },
        "id": "5eFMFwVlna4o",
        "outputId": "304b935b-52e8-4fec-9c39-b06a1a87d1d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-0c60bafd-6be2-4b5c-96ae-6f1a8855f3c6\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-0c60bafd-6be2-4b5c-96ae-6f1a8855f3c6\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.1 Enter a single sequence to be tested, and select a code cell to run as needed(1.1 or 1.2).\n",
        "single = 1\n",
        "single_test_sequence = ['MSGGGVIRGPAGNNDCRIYVGNLPPDIRTKDIEDVFYKYGAIRDIDLKNRRGGPPFAFVEFEDPRDAEDAVYGRDGYDYDGYRLRVEFPRSGRGTGRGGGGGGGGGAPRGRYGPPSRRSENRVVVSGLPPSGSWQDLKDHMREAGDVCYADVYRDGTGVVEFVRKEDMTYAVRKLDNTKFRSHEGETAYIRVKVDGPRSPSYGRSRSRSRSRSRSRSRSNSRSRSYSPRRSRGSPRYSPRHSRSRSRT'] # please enter your protein sequence\n",
        "\n",
        "\n",
        "pos_test_sequence = [protein.lower() for protein in single_test_sequence]\n",
        "neg_test_sequence = [protein.lower() for protein in single_test_sequence]"
      ],
      "metadata": {
        "id": "PFUNOqZLUdTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.2 Enter the file address of the txt file containing the test sequences, one sequence per line, please select a code cell to run as needed(1.1 or 1.2).\n",
        "import pandas as pd\n",
        "\n",
        "pos_test_dir = \"pos_word_list_mydata_test.txt\" # Needs to be modified to your test dataset file\n",
        "neg_test_dir = \"\"\n",
        "single = 0\n",
        "def readdata_test(pos_protein_dir, neg_protein_dir):\n",
        "    pos_protein_path = pos_protein_dir\n",
        "    neg_protein_path = neg_protein_dir\n",
        "    if pos_protein_path == '':\n",
        "        pos_word_list = []\n",
        "    else:\n",
        "      with open(pos_protein_path, 'r') as f:\n",
        "          pos_word_list = f.read().splitlines()\n",
        "\n",
        "    if neg_protein_path == '':\n",
        "        neg_word_list = []\n",
        "    else:\n",
        "      with open(neg_protein_path, 'r') as f:\n",
        "          neg_word_list = f.read().splitlines()\n",
        "\n",
        "    pos_sequence = [protein.lower() for protein in pos_word_list]\n",
        "    neg_sequence = [protein.lower() for protein in neg_word_list]\n",
        "    return pos_sequence, neg_sequence\n",
        "\n",
        "pos_test_sequence,neg_test_sequence = readdata_test(pos_test_dir, neg_test_dir)"
      ],
      "metadata": {
        "id": "WhhD8vtJTN4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.Define the model\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data import dataset, dataloader\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from torch import LongTensor, Tensor, from_numpy, max_pool1d, nn, unsqueeze,optim\n",
        "import argparse\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn import metrics\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import pandas as pd\n",
        "import copy\n",
        "import random\n",
        "def readdata(pos_protein_dir, neg_protein_dir, length, pos_seed, neg_seed):\n",
        "    pos_protein_path = pos_protein_dir\n",
        "    neg_protein_path = neg_protein_dir\n",
        "    with open(pos_protein_path, 'r') as f:\n",
        "        pos_word_list = f.read().splitlines()\n",
        "    f.close\n",
        "    with open(neg_protein_path, 'r') as f:\n",
        "        neg_word_list = f.read().splitlines()\n",
        "    f.close\n",
        "\n",
        "    np.random.seed(pos_seed)\n",
        "    np.random.shuffle(pos_word_list)\n",
        "    np.random.seed(neg_seed)\n",
        "    np.random.shuffle(neg_word_list)\n",
        "    if length is not None:\n",
        "        neg_word_list = neg_word_list[:length]\n",
        "        pos_word_list = pos_word_list[:length]\n",
        "    pos_sequence = pos_word_list\n",
        "    neg_sequence = neg_word_list\n",
        "    return pos_sequence, neg_sequence\n",
        "\n",
        "\n",
        "\n",
        "def word2Num(train, test, min=0, max=None, max_features=None):\n",
        "    dic = {}\n",
        "    count = {}\n",
        "    for list0 in train:\n",
        "        list0 = list0.replace(' ', '')\n",
        "        for word in list0:\n",
        "            count[word] = count.get(word, 0) + 1\n",
        "    if min is not None:\n",
        "        count = {word:value for word,value in count.items() if value>min}\n",
        "    if max is not None:\n",
        "        count = {word:value for word,value in count.items() if value<max}\n",
        "    if  max_features is not None:\n",
        "        temp = sorted(count.items(), key=lambda x:x[-1], reverse=True)[:max_features]\n",
        "        count = dict(temp)\n",
        "    for word in count:\n",
        "        dic[word] = len(dic) + 1\n",
        "    print(dic)\n",
        "    Num = []\n",
        "    for list0 in train:\n",
        "        list0 = list0.replace(' ', '')\n",
        "        num = []\n",
        "        for word in list0:\n",
        "            num.append(dic.get(word))\n",
        "        Num.append(num)\n",
        "    print(len(Num))\n",
        "    Num2 = []\n",
        "    for list0 in test:\n",
        "        list0 = list0.replace(' ', '')\n",
        "        num2 = []\n",
        "        for word in list0:\n",
        "            num2.append(dic.get(word))\n",
        "        Num2.append(num2)\n",
        "    print(len(Num2))\n",
        "    return Num, Num2, dic\n",
        "\n",
        "\n",
        "def collate_fn(data):\n",
        "    data.sort(key=lambda tuple: len(tuple[0]), reverse=True)\n",
        "    data_length = [len(tuple[0]) for tuple in data]\n",
        "    data_ten, data_label = [], []\n",
        "    for tuple in data:\n",
        "        data_ten.append(tuple[0])\n",
        "        data_label.append(tuple[1])\n",
        "    data_ten = pad_sequence(data_ten, batch_first=True,padding_value=0)\n",
        "    data_label = torch.LongTensor(data_label)\n",
        "    data_length = torch.LongTensor(data_length)\n",
        "    return data_ten, data_label, data_length\n",
        "\n",
        "\n",
        "class Mydata(dataset.Dataset):\n",
        "    def __init__(self, data, label):\n",
        "        self.data = data\n",
        "        self.label = label\n",
        "    def __getitem__(self, idx):\n",
        "        protein = self.data[idx]\n",
        "        label = self.label[idx]\n",
        "        return protein, label\n",
        "    def __len__(self):\n",
        "        assert len(self.data)==len(self.label)\n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "class ECALayer(nn.Module):\n",
        "    \"\"\"Constructs a ECA module.\n",
        "    Args:\n",
        "        channel: Number of channels of the input feature map\n",
        "        k_size: Adaptive selection of kernel size\n",
        "    \"\"\"\n",
        "    def __init__(self, k_size=5): # 3 5\n",
        "        super(ECALayer, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.conv = nn.Conv1d(1, 1, kernel_size=k_size, padding=(k_size - 1) // 2, bias=False)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x, length):\n",
        "        # feature descriptor on the global spatial information\n",
        "        b, e, t = x.size()\n",
        "\n",
        "        for i in range(b):\n",
        "            x_pack = x[i][: , : length[i]].unsqueeze(0)\n",
        "            x_avg = self.avg_pool(x_pack)\n",
        "            if i == 0:\n",
        "                y = x_avg.clone()\n",
        "            else:\n",
        "                y = torch.cat((y, x_avg), dim=0)\n",
        "\n",
        "        # Two different branches of ECA module\n",
        "        y = self.conv(y.transpose(-1, -2)).transpose(-1, -2)\n",
        "\n",
        "        # Multi-scale information fusion\n",
        "        y = self.sigmoid(y)\n",
        "\n",
        "        return x * y.expand_as(x)\n",
        "\n",
        "\n",
        "class RCNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_num, hidden_dim, num_layers, biFlag, dropout=0.2):\n",
        "        super(RCNN, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_num = embedding_num\n",
        "        self.hiddern_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        if biFlag:\n",
        "            self.bi_num = 2\n",
        "        else:\n",
        "            self.bi_num = 1\n",
        "        self.biFlag = biFlag\n",
        "        self.device = torch.device(\"cuda\")\n",
        "        self.ECABlock= ECALayer()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_num, padding_idx=0)   # 需要添加padding_idx\n",
        "        self.lstm = nn.LSTM(input_size= embedding_num, hidden_size=hidden_dim, num_layers=num_layers, batch_first=True, bidirectional=biFlag)\n",
        "        self.globalmaxpool = nn.AdaptiveMaxPool1d(1)\n",
        "        self.linear = nn.Sequential(\n",
        "            nn.Dropout(dropout),\n",
        "\n",
        "            nn.Linear(self.bi_num*hidden_dim + embedding_num, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(128,32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32,2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, length):\n",
        "        embed = self.embedding(x)\n",
        "        x = pack_padded_sequence(embed, length.cpu(), batch_first=True)\n",
        "        x, (ht,ct) = self.lstm(x)\n",
        "        out, out_len = pad_packed_sequence(x, batch_first=True)\n",
        "        out = torch.cat((embed, out), 2)\n",
        "        out = F.relu(out)\n",
        "        out = out.permute(0, 2, 1)\n",
        "\n",
        "        out1 = self.ECABlock(out, length)\n",
        "        out = out + out1\n",
        "\n",
        "        out = self.globalmaxpool(out).squeeze()\n",
        "        out = F.relu(out)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def set_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False"
      ],
      "metadata": {
        "id": "Bjmxy3bGMeXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.running model\n",
        "if __name__== '__main__':\n",
        "    device = torch.device(\"cpu\")\n",
        "    seed = 1\n",
        "    set_seed(seed)\n",
        "    pos_protein_dir = 'pos_word_list_mydata_all_1507.txt'\n",
        "    neg_protein_dir = 'neg_word_list_1479.txt'\n",
        "    model_path = 'model_mydata_1.pt'\n",
        "    list_length = 1479\n",
        "\n",
        "    pos_seed_list = [20]\n",
        "    neg_seed_list = [21]\n",
        "\n",
        "\n",
        "    for i in range(len(pos_seed_list)):\n",
        "        pos_seed = pos_seed_list[i]\n",
        "        neg_seed = neg_seed_list[i]\n",
        "        pos_sequence, neg_sequence = readdata(pos_protein_dir, neg_protein_dir, list_length, pos_seed, neg_seed)\n",
        "\n",
        "        if not os.path.exists(\"classification_output/personal_output\"):\n",
        "            os.makedirs(\"classification_output/personal_output\")\n",
        "        # auc_save_csv = 'classification_output/personal_output/personal_test_roc_{}.csv'.format((i+1))\n",
        "        result_save_csv = 'classification_output/personal_output/result_{}.csv'.format((i+1))\n",
        "        # df_test = pd.DataFrame(columns=['y_true', 'y_score'])\n",
        "        # df_test.to_csv(auc_save_csv, mode='w', index=False)\n",
        "        df_test = pd.DataFrame(columns=['acc', 'sen', 'spe', 'auc'])\n",
        "        df_test.to_csv(result_save_csv, index=False)\n",
        "\n",
        "        neg_num = len(neg_test_sequence)\n",
        "        pos_num = len(pos_test_sequence)\n",
        "        print('pos_num=',pos_num)\n",
        "        print('neg_num=',neg_num)\n",
        "\n",
        "        neg_num = len(neg_sequence)\n",
        "        pos_num = len(pos_sequence)\n",
        "\n",
        "        start = 0\n",
        "        interval = 0.1\n",
        "        val_split = 0.1\n",
        "\n",
        "        total_tp = 0\n",
        "        total_p = 0\n",
        "        total_n = 0\n",
        "        total_tn = 0\n",
        "\n",
        "        save_sen = []\n",
        "        save_spe = []\n",
        "        save_acc = []\n",
        "\n",
        "        fold = 0\n",
        "        total_correct, total_F1, total_R, total_precision = [],[],[],[]\n",
        "\n",
        "\n",
        "        test_pos_seq = pos_test_sequence\n",
        "        test_neg_seq = neg_test_sequence\n",
        "\n",
        "        train_val_pos_seq = pos_sequence[:int(pos_num*start)] + pos_sequence[int(pos_num*(start+interval)):]\n",
        "        train_val_neg_seq = neg_sequence[:int(neg_num * start)] + neg_sequence[int(neg_num * (start + interval)):]\n",
        "        train_val_pos_num = len(train_val_pos_seq)\n",
        "        train_val_neg_num = len(train_val_neg_seq)\n",
        "        np.random.shuffle(train_val_pos_seq)\n",
        "        np.random.shuffle(train_val_neg_seq)\n",
        "        val_pos_seq = train_val_pos_seq[:int(train_val_pos_num*val_split)]\n",
        "        train_pos_seq = train_val_pos_seq[int(train_val_pos_num*val_split):]\n",
        "        val_neg_seq = train_val_neg_seq[:int(train_val_neg_num*val_split)]\n",
        "        train_neg_seq = train_val_neg_seq[int(train_val_neg_num*val_split):]\n",
        "\n",
        "        test_y = np.hstack((np.zeros(shape=(len(test_neg_seq), )),\n",
        "                        np.ones(shape=(len(test_pos_seq), ))))\n",
        "\n",
        "        print('test_pos', test_y[test_y == 1].shape)\n",
        "        print('test_neg', test_y[test_y == 0].shape)\n",
        "\n",
        "        if (len(test_neg_seq) and len(test_pos_seq)):\n",
        "          flag = 1\n",
        "        else:\n",
        "          flag = 0\n",
        "\n",
        "        train_seq = train_neg_seq + train_pos_seq\n",
        "        val_seq = val_neg_seq + val_pos_seq\n",
        "        train_val_seq = train_seq + val_seq\n",
        "        test_seq = test_neg_seq + test_pos_seq\n",
        "\n",
        "        state = np.random.get_state()\n",
        "        np.random.shuffle(test_seq)\n",
        "        np.random.set_state(state)\n",
        "        np.random.shuffle(test_y)\n",
        "\n",
        "        _, test_num, vocab  = word2Num(train_seq, test_seq)\n",
        "        test_data_size = len(test_num)\n",
        "\n",
        "\n",
        "        test_ten = []\n",
        "        for list1 in test_num:\n",
        "            test_ten.append(torch.LongTensor(list1))\n",
        "\n",
        "        test_label_ten = from_numpy(test_y)\n",
        "        test_label_ten = test_label_ten.type(torch.LongTensor)\n",
        "\n",
        "        state_dict = torch.load(model_path, map_location=torch.device('cpu'))\n",
        "        rcnn = RCNN(len(vocab)+1, 1024, 100, 1, True)\n",
        "        rcnn = rcnn.to(device)\n",
        "        rcnn.load_state_dict(state_dict)\n",
        "        rcnn.eval()\n",
        "        print(rcnn)\n",
        "\n",
        "        test = Mydata(test_ten, test_label_ten)\n",
        "\n",
        "        set_seed(seed)\n",
        "        test_dataloader = dataloader.DataLoader(dataset=test, batch_size=32,shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "        test_loss = 0\n",
        "        y_true_test = []\n",
        "        y_pre_test = []\n",
        "        y_score_test = []\n",
        "        total_labels_test = 0\n",
        "        with torch.no_grad():\n",
        "            for input, label, length in test_dataloader:\n",
        "                input = input.to(device)\n",
        "                label = label.to(device)\n",
        "                length = length.to(device)\n",
        "\n",
        "                output = rcnn(input, length)\n",
        "                _, predicted = torch.max(output,1)\n",
        "                y_pre_test.extend(predicted.cpu())\n",
        "                y_true_test.extend(label.cpu())\n",
        "                y_score_test.extend(torch.softmax(output, dim=-1)[:, 1].cpu().detach())\n",
        "                total_labels_test += label.size(0)\n",
        "\n",
        "                test_correct = metrics.accuracy_score(y_true_test, y_pre_test)\n",
        "                test_F1 = metrics.f1_score(y_true_test, y_pre_test, average='macro')\n",
        "                test_R = metrics.recall_score(y_true_test, y_pre_test)\n",
        "                test_precision = metrics.precision_score(y_true_test, y_pre_test)\n",
        "                if flag and (not single):\n",
        "                  test_auc = metrics.roc_auc_score(y_true_test, y_score_test)\n",
        "\n",
        "                  save_content = 'Test: Correct: %.5f, Precision: %.5f, R: %.5f, F1(macro): %.5f, AUC:%.5f, test_loss: %f' % \\\n",
        "                              (test_correct, test_precision, test_R, test_F1, test_auc, test_loss)\n",
        "                  print(save_content)\n",
        "\n",
        "            y_true_data = [i.item() for i in y_true_test]\n",
        "            y_score_data = [i.item() for i in y_score_test]\n",
        "            y_pre_data = [i.item() for i in y_pre_test]\n",
        "            if single:\n",
        "              print({'Seq':test_seq[0], 'y_pre': np.array(y_pre_data)[0], 'y_score':y_score_data[0]})\n",
        "            else:\n",
        "              if not os.path.exists(\"classification_output/personal_output\"):\n",
        "                  os.makedirs(\"classification_output/personal_output\")\n",
        "              auc_save_csv = 'classification_output/personal_output/personal_test_roc_{}.csv'.format((i+1))\n",
        "              df_test = pd.DataFrame(columns=['Seq', 'y_true', 'y_score', 'y_pre'])\n",
        "              df_test.to_csv(auc_save_csv,mode='w', index=False)\n",
        "              auc_dict = {'Seq':test_seq , 'y_true':y_true_data, 'y_pre': np.array(y_pre_data), 'y_score':y_score_data}\n",
        "              auc_score = pd.DataFrame(auc_dict)\n",
        "              auc_score.to_csv(auc_save_csv, mode='a', header=False, index=False, float_format='%.4f')\n",
        "\n",
        "              p = np.array(y_pre_data)[np.array(y_true_data) == 1]\n",
        "              tp = p[p == 1]\n",
        "              n = np.array(y_pre_data)[np.array(y_true_data) == 0]\n",
        "              tn = n[n == 0]\n",
        "\n",
        "              sen = tp.shape[0] / p.shape[0] if p.shape[0] > 0 else 1\n",
        "              spe = tn.shape[0] / n.shape[0] if n.shape[0] > 0 else 1\n",
        "              acc = (tp.shape[0] + tn.shape[0]) / (p.shape[0] + n.shape[0])\n",
        "              if flag:\n",
        "                auc = metrics.roc_auc_score(y_true_data, y_score_data)\n",
        "              else:\n",
        "                auc = 0\n",
        "              print('sen:', sen)\n",
        "              print('spe:', spe)\n",
        "              print('acc:', acc)\n",
        "              print('auc:', auc)\n",
        "\n",
        "              # list1 = [test_loss, test_correct, test_F1, test_R, test_precision, (tn.shape[0] / n.shape[0] if n.shape[0] > 0 else 1), test_auc]\n",
        "              test_dict = {'acc':[acc], 'sen':[sen], 'spe':[spe], 'auc':[auc]}\n",
        "              # list.extend(list1)\n",
        "              # data_test = pd.DataFrame([list])\n",
        "              test_score = pd.DataFrame(test_dict)\n",
        "              print(test_score)\n",
        "              test_score.to_csv(result_save_csv, mode='a', header=False, index=False, float_format='%.4f')"
      ],
      "metadata": {
        "id": "PaScVPYNMMaq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13948d48-3652-43eb-ad7e-f00eb0b750df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pos_num= 147\n",
            "neg_num= 0\n",
            "test_pos (147,)\n",
            "test_neg (0,)\n",
            "{'m': 1, 's': 2, 'w': 3, 'l': 4, 't': 5, 'e': 6, 'd': 7, 'i': 8, 'r': 9, 'g': 10, 'f': 11, 'y': 12, 'k': 13, 'a': 14, 'h': 15, 'p': 16, 'c': 17, 'v': 18, 'n': 19, 'q': 20}\n",
            "2398\n",
            "147\n",
            "RCNN(\n",
            "  (ECABlock): ECALayer(\n",
            "    (avg_pool): AdaptiveAvgPool1d(output_size=1)\n",
            "    (conv): Conv1d(1, 1, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
            "    (sigmoid): Sigmoid()\n",
            "  )\n",
            "  (embedding): Embedding(21, 1024, padding_idx=0)\n",
            "  (lstm): LSTM(1024, 100, batch_first=True, bidirectional=True)\n",
            "  (globalmaxpool): AdaptiveMaxPool1d(output_size=1)\n",
            "  (linear): Sequential(\n",
            "    (0): Dropout(p=0.2, inplace=False)\n",
            "    (1): Linear(in_features=1224, out_features=128, bias=True)\n",
            "    (2): ReLU()\n",
            "    (3): Dropout(p=0.2, inplace=False)\n",
            "    (4): Linear(in_features=128, out_features=32, bias=True)\n",
            "    (5): ReLU()\n",
            "    (6): Linear(in_features=32, out_features=2, bias=True)\n",
            "  )\n",
            ")\n",
            "sen: 0.891156462585034\n",
            "spe: 1\n",
            "acc: 0.891156462585034\n",
            "auc: 0\n",
            "        acc       sen  spe  auc\n",
            "0  0.891156  0.891156    1    0\n"
          ]
        }
      ]
    }
  ]
}