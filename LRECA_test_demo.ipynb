{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1A2R2Lk42hZ1tnaIRzLhTFyMjdelsLtEr",
      "authorship_tag": "ABX9TyNMUrWJdGf3kLUNtKWpNYHi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ai-phasepro/LRECA/blob/main/LRECA_test_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LRECA test Demo**\n",
        "\n",
        "## **Preparation**\n",
        "\n",
        "**Before you start using the demos, please run code cell 0.1 and code cell 0.2 first, which you only need to run once if they run with no error.**\n",
        "\n",
        "## **Test Data Input**\n",
        "\n",
        "Then choose to run **code cell 1.1 or code cell 1.2** according to your test data, If you test **a single sequence**, please replace the sequence **‘MSGGG...RSRT’** in the code of code cell 1.1 with your sequence, and then run **code cell 1.1**.\n",
        "\n",
        "if you test **multiple sequences**, please **upload the txt file** of one sequence per line to colab and **modify the path(pos_test_dir & neg_test_dir)** of the test file to **run code cell 1.2**.\n",
        "\n",
        "## **Define the model and run the tests**\n",
        "\n",
        "Then, run **code cell 2** to define the model (which is usually only run once when testing multiple times in a runtime),\n",
        "\n",
        "Finally, run **code cell 3** to **run the model** and output the test results.\n",
        "\n",
        "## **output**\n",
        "\n",
        "For **single** sequences the test results are displayed in the the last line  of the code cell 3.The output of the example is “'y_score': 0.9564854502677917, 'y_pre': 1”, in which ‘y_score’ is the predicted value of the input sequence and ‘y_pre’ is the predicted label (1 represents LLPS positive, and 0 represents LLPS negative).\n",
        "\n",
        "For test results for **multiple** sequences,\n",
        "\n",
        "the **predicted results for each sequence** are output in **classification_output/personal_output/personal_test_roc_1.csv **\n",
        "\n",
        "the **test metrics** are output in\n",
        "**classification_output/personal_output/result_1.csv**\n",
        "Note: the output test metrics in result_1.csv are meaningful only when the sequences in the upload files under the ‘pos_test_dir’ and ‘neg_test_dir’ paths have been determined to be LLPS positive and negative, respectively. Otherwise, the result_1.csv has no reference value.\n",
        "\n",
        "\n",
        "## **help**\n",
        "\n",
        "### **How to upload a test fileset to colab**\n",
        "\n",
        "Here are two ways to upload a test dataset file.\n",
        "\n",
        "1. Click on the **\"File\" icon** on **the left sidebar**, and then click on \"**Upload to session storge**\" at the top of the pop-up file sidebar to upload a file.\n",
        "\n",
        "2. Run **code cell 0.3** and select file upload\n",
        "\n",
        "See the official [Google Colab instructions](https://colab.research.google.com/notebooks/io.ipynb#scrollTo=7Z2jcRKwUHqV) for more file uploading options\n",
        "\n",
        "### **When testing multiple sequences using the txt file format, do I have to split the positive and negative data to upload?**\n",
        "\n",
        "**Not required**, distinguishing between positives and negatives for test set files is a performance metric that facilitates statistical modeling. If you are testing an unlabeled dataset or just want to see the predictions of the sequences, **you can set only one of the paths (pos_test_dir / neg_test_dir) to the path of the file to be tested and set the other to an empty string (\"\")**, in **which case the performance metrics are meaningless**. In this case, the performance metrics are meaningless. You can see the results for each sequence in the sequence prediction output file (classification_output/personal_output/personal_test_roc_1.csv).\n",
        "\n"
      ],
      "metadata": {
        "id": "OhNYlFAvyulA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 0.1 First run the code grid to download the data needed for the demo and unzip it.\n",
        "!gdown \"https://drive.google.com/uc?export=download&id=1AfIylZdDgd7yZYD2seVw9998SpBVM0BW\"\n",
        "!unzip LRECA_testdata.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0hpbOEKiX1bp",
        "outputId": "06c8f5cc-e324-46a5-cd2e-f77b87994397"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?export=download&id=1AfIylZdDgd7yZYD2seVw9998SpBVM0BW\n",
            "To: /content/LRECA_testdata.zip\n",
            "\r  0% 0.00/4.92M [00:00<?, ?B/s]\r100% 4.92M/4.92M [00:00<00:00, 106MB/s]\n",
            "Archive:  LRECA_testdata.zip\n",
            "  inflating: model_mydata_1.pt       \n",
            "  inflating: neg_word_list_1479.txt  \n",
            "  inflating: neg_word_list_mydata_test.txt  \n",
            "  inflating: pos_word_list_mydata_all_1507.txt  \n",
            "  inflating: pos_word_list_mydata_test.txt  \n",
            "  inflating: requirements.txt        \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 0.2 Configure the runtime environment\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu # if using GPU runtime,please comment out this line\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "h8ugGI1pM00k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ea5c07d-64ef-4bf3-f43e-04edaa320585",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cpu\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.18.1+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.3.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "INFO: pip is looking at multiple versions of torch to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torch\n",
            "  Downloading https://download.pytorch.org/whl/cpu/torch-2.4.0%2Bcpu-cp310-cp310-linux_x86_64.whl (195.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.0/195.0 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "  Downloading https://download.pytorch.org/whl/cpu/torch-2.3.1%2Bcpu-cp310-cp310-linux_x86_64.whl (190.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.4/190.4 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.3.1+cu121\n",
            "    Uninstalling torch-2.3.1+cu121:\n",
            "      Successfully uninstalled torch-2.3.1+cu121\n",
            "Successfully installed torch-2.3.1+cpu\n",
            "Collecting asttokens==2.4.1 (from -r requirements.txt (line 1))\n",
            "  Downloading asttokens-2.4.1-py2.py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: backcall==0.2.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (0.2.0)\n",
            "Collecting bio==1.6.0 (from -r requirements.txt (line 3))\n",
            "  Downloading bio-1.6.0-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting biopython==1.81 (from -r requirements.txt (line 4))\n",
            "  Downloading biopython-1.81-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Collecting biothings-client==0.3.1 (from -r requirements.txt (line 5))\n",
            "  Downloading biothings_client-0.3.1-py2.py3-none-any.whl.metadata (9.8 kB)\n",
            "Collecting charset-normalizer==2.1.1 (from -r requirements.txt (line 6))\n",
            "  Downloading charset_normalizer-2.1.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting comm==0.2.0 (from -r requirements.txt (line 7))\n",
            "  Downloading comm-0.2.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting contourpy==1.1.1 (from -r requirements.txt (line 8))\n",
            "  Downloading contourpy-1.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: cycler==0.12.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (0.12.1)\n",
            "Requirement already satisfied: et-xmlfile==1.1.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (1.1.0)\n",
            "Collecting executing==2.0.1 (from -r requirements.txt (line 11))\n",
            "  Downloading executing-2.0.1-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting filelock==3.9.0 (from -r requirements.txt (line 12))\n",
            "  Downloading filelock-3.9.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting fonttools==4.45.0 (from -r requirements.txt (line 13))\n",
            "  Downloading fonttools-4.45.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.0/155.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gensim==4.3.2 (from -r requirements.txt (line 14))\n",
            "  Downloading gensim-4.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
            "Collecting gprofiler-official==1.0.0 (from -r requirements.txt (line 15))\n",
            "  Downloading gprofiler_official-1.0.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting idna==3.4 (from -r requirements.txt (line 16))\n",
            "  Downloading idna-3.4-py3-none-any.whl.metadata (9.8 kB)\n",
            "Collecting importlib-metadata==6.9.0 (from -r requirements.txt (line 17))\n",
            "  Downloading importlib_metadata-6.9.0-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting importlib-resources==6.1.1 (from -r requirements.txt (line 18))\n",
            "  Downloading importlib_resources-6.1.1-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting jedi==0.19.1 (from -r requirements.txt (line 19))\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting Jinja2==3.1.2 (from -r requirements.txt (line 20))\n",
            "  Downloading Jinja2-3.1.2-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting joblib==1.3.2 (from -r requirements.txt (line 21))\n",
            "  Downloading joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting jupyter_core==5.5.0 (from -r requirements.txt (line 22))\n",
            "  Downloading jupyter_core-5.5.0-py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: kiwisolver==1.4.5 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 23)) (1.4.5)\n",
            "Collecting MarkupSafe==2.1.3 (from -r requirements.txt (line 24))\n",
            "  Downloading MarkupSafe-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: mpmath==1.3.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 25)) (1.3.0)\n",
            "Collecting mygene==3.2.2 (from -r requirements.txt (line 26))\n",
            "  Downloading mygene-3.2.2-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Collecting nest-asyncio==1.5.8 (from -r requirements.txt (line 27))\n",
            "  Downloading nest_asyncio-1.5.8-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting networkx==3.0 (from -r requirements.txt (line 28))\n",
            "  Downloading networkx-3.0-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting openpyxl==3.1.2 (from -r requirements.txt (line 29))\n",
            "  Downloading openpyxl-3.1.2-py2.py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting packaging==23.2 (from -r requirements.txt (line 30))\n",
            "  Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: pandas==2.0.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 31)) (2.0.3)\n",
            "Collecting parso==0.8.3 (from -r requirements.txt (line 32))\n",
            "  Downloading parso-0.8.3-py2.py3-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: pexpect==4.9.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 33)) (4.9.0)\n",
            "Collecting pickle5==0.0.11 (from -r requirements.txt (line 34))\n",
            "  Downloading pickle5-0.0.11.tar.gz (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.1/132.1 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pickleshare==0.7.5 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 35)) (0.7.5)\n",
            "Collecting platformdirs==4.0.0 (from -r requirements.txt (line 36))\n",
            "  Downloading platformdirs-4.0.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting pooch==1.8.0 (from -r requirements.txt (line 37))\n",
            "  Downloading pooch-1.8.0-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: ptyprocess==0.7.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 38)) (0.7.0)\n",
            "Collecting pure-eval==0.2.2 (from -r requirements.txt (line 39))\n",
            "  Downloading pure_eval-0.2.2-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting Pygments==2.17.2 (from -r requirements.txt (line 40))\n",
            "  Downloading pygments-2.17.2-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting pyparsing==3.1.1 (from -r requirements.txt (line 41))\n",
            "  Downloading pyparsing-3.1.1-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: python-dateutil==2.8.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 42)) (2.8.2)\n",
            "Collecting pytz==2023.3.post1 (from -r requirements.txt (line 43))\n",
            "  Downloading pytz-2023.3.post1-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: scikit-learn==1.3.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 44)) (1.3.2)\n",
            "Collecting scipy==1.10.1 (from -r requirements.txt (line 45))\n",
            "  Downloading scipy-1.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.9/58.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting seaborn==0.13.0 (from -r requirements.txt (line 46))\n",
            "  Downloading seaborn-0.13.0-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting SimpleITK==2.3.0 (from -r requirements.txt (line 47))\n",
            "  Downloading SimpleITK-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: six==1.16.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 48)) (1.16.0)\n",
            "Collecting smart-open==6.4.0 (from -r requirements.txt (line 49))\n",
            "  Downloading smart_open-6.4.0-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting stack-data==0.6.3 (from -r requirements.txt (line 50))\n",
            "  Downloading stack_data-0.6.3-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting sympy==1.12 (from -r requirements.txt (line 51))\n",
            "  Downloading sympy-1.12-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting threadpoolctl==3.2.0 (from -r requirements.txt (line 52))\n",
            "  Downloading threadpoolctl-3.2.0-py3-none-any.whl.metadata (10.0 kB)\n",
            "Collecting tqdm==4.66.1 (from -r requirements.txt (line 53))\n",
            "  Downloading tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting traitlets==5.14.0 (from -r requirements.txt (line 54))\n",
            "  Downloading traitlets-5.14.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting tzdata==2023.3 (from -r requirements.txt (line 55))\n",
            "  Downloading tzdata-2023.3-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting urllib3==1.26.13 (from -r requirements.txt (line 56))\n",
            "  Downloading urllib3-1.26.13-py2.py3-none-any.whl.metadata (47 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.5/47.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: xlrd==2.0.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 57)) (2.0.1)\n",
            "Collecting zipp==3.17.0 (from -r requirements.txt (line 58))\n",
            "  Downloading zipp-3.17.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bio==1.6.0->-r requirements.txt (line 3)) (2.31.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from biopython==1.81->-r requirements.txt (line 4)) (1.25.2)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.3 in /usr/local/lib/python3.10/dist-packages (from seaborn==0.13.0->-r requirements.txt (line 46)) (3.7.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.3->seaborn==0.13.0->-r requirements.txt (line 46)) (9.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bio==1.6.0->-r requirements.txt (line 3)) (2024.7.4)\n",
            "Downloading asttokens-2.4.1-py2.py3-none-any.whl (27 kB)\n",
            "Downloading bio-1.6.0-py3-none-any.whl (279 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m279.4/279.4 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading biopython-1.81-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading biothings_client-0.3.1-py2.py3-none-any.whl (29 kB)\n",
            "Downloading charset_normalizer-2.1.1-py3-none-any.whl (39 kB)\n",
            "Downloading comm-0.2.0-py3-none-any.whl (7.0 kB)\n",
            "Downloading contourpy-1.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.7/301.7 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading executing-2.0.1-py2.py3-none-any.whl (24 kB)\n",
            "Downloading filelock-3.9.0-py3-none-any.whl (9.7 kB)\n",
            "Downloading fonttools-4.45.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gensim-4.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.5/26.5 MB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gprofiler_official-1.0.0-py3-none-any.whl (9.3 kB)\n",
            "Downloading idna-3.4-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading importlib_metadata-6.9.0-py3-none-any.whl (22 kB)\n",
            "Downloading importlib_resources-6.1.1-py3-none-any.whl (33 kB)\n",
            "Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m46.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.1/133.1 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.2/302.2 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyter_core-5.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading MarkupSafe-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Downloading mygene-3.2.2-py2.py3-none-any.whl (5.4 kB)\n",
            "Downloading nest_asyncio-1.5.8-py3-none-any.whl (5.3 kB)\n",
            "Downloading networkx-3.0-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openpyxl-3.1.2-py2.py3-none-any.whl (249 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.0/250.0 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading parso-0.8.3-py2.py3-none-any.whl (100 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.8/100.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading platformdirs-4.0.0-py3-none-any.whl (17 kB)\n",
            "Downloading pooch-1.8.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pure_eval-0.2.2-py3-none-any.whl (11 kB)\n",
            "Downloading pygments-2.17.2-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyparsing-3.1.1-py3-none-any.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.1/103.1 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytz-2023.3.post1-py2.py3-none-any.whl (502 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m502.5/502.5 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.4/34.4 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading seaborn-0.13.0-py3-none-any.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading SimpleITK-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (52.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.6/52.6 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smart_open-6.4.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading stack_data-0.6.3-py3-none-any.whl (24 kB)\n",
            "Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
            "Downloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading traitlets-5.14.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.2/85.2 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.8/341.8 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading urllib3-1.26.13-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading zipp-3.17.0-py3-none-any.whl (7.4 kB)\n",
            "Building wheels for collected packages: pickle5\n",
            "  Building wheel for pickle5 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pickle5: filename=pickle5-0.0.11-cp310-cp310-linux_x86_64.whl size=255312 sha256=9ccbc82e92d7583ddbe56873514449e167a7b1fbd3309d04120f1afdf778180b\n",
            "  Stored in directory: /root/.cache/pip/wheels/7d/14/ef/4aab19d27fa8e58772be5c71c16add0426acf9e1f64353235c\n",
            "Successfully built pickle5\n",
            "Installing collected packages: SimpleITK, pytz, pure-eval, pickle5, zipp, urllib3, tzdata, traitlets, tqdm, threadpoolctl, sympy, smart-open, scipy, pyparsing, Pygments, platformdirs, parso, packaging, openpyxl, networkx, nest-asyncio, MarkupSafe, joblib, importlib-resources, idna, fonttools, filelock, executing, contourpy, charset-normalizer, biopython, asttokens, stack-data, jupyter_core, Jinja2, jedi, importlib-metadata, gensim, comm, seaborn, pooch, gprofiler-official, biothings-client, mygene, bio\n",
            "  Attempting uninstall: pytz\n",
            "    Found existing installation: pytz 2024.1\n",
            "    Uninstalling pytz-2024.1:\n",
            "      Successfully uninstalled pytz-2024.1\n",
            "  Attempting uninstall: zipp\n",
            "    Found existing installation: zipp 3.19.2\n",
            "    Uninstalling zipp-3.19.2:\n",
            "      Successfully uninstalled zipp-3.19.2\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.0.7\n",
            "    Uninstalling urllib3-2.0.7:\n",
            "      Successfully uninstalled urllib3-2.0.7\n",
            "  Attempting uninstall: tzdata\n",
            "    Found existing installation: tzdata 2024.1\n",
            "    Uninstalling tzdata-2024.1:\n",
            "      Successfully uninstalled tzdata-2024.1\n",
            "  Attempting uninstall: traitlets\n",
            "    Found existing installation: traitlets 5.7.1\n",
            "    Uninstalling traitlets-5.7.1:\n",
            "      Successfully uninstalled traitlets-5.7.1\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.66.4\n",
            "    Uninstalling tqdm-4.66.4:\n",
            "      Successfully uninstalled tqdm-4.66.4\n",
            "  Attempting uninstall: threadpoolctl\n",
            "    Found existing installation: threadpoolctl 3.5.0\n",
            "    Uninstalling threadpoolctl-3.5.0:\n",
            "      Successfully uninstalled threadpoolctl-3.5.0\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.13.1\n",
            "    Uninstalling sympy-1.13.1:\n",
            "      Successfully uninstalled sympy-1.13.1\n",
            "  Attempting uninstall: smart-open\n",
            "    Found existing installation: smart-open 7.0.4\n",
            "    Uninstalling smart-open-7.0.4:\n",
            "      Successfully uninstalled smart-open-7.0.4\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.13.1\n",
            "    Uninstalling scipy-1.13.1:\n",
            "      Successfully uninstalled scipy-1.13.1\n",
            "  Attempting uninstall: pyparsing\n",
            "    Found existing installation: pyparsing 3.1.2\n",
            "    Uninstalling pyparsing-3.1.2:\n",
            "      Successfully uninstalled pyparsing-3.1.2\n",
            "  Attempting uninstall: Pygments\n",
            "    Found existing installation: Pygments 2.16.1\n",
            "    Uninstalling Pygments-2.16.1:\n",
            "      Successfully uninstalled Pygments-2.16.1\n",
            "  Attempting uninstall: platformdirs\n",
            "    Found existing installation: platformdirs 4.2.2\n",
            "    Uninstalling platformdirs-4.2.2:\n",
            "      Successfully uninstalled platformdirs-4.2.2\n",
            "  Attempting uninstall: parso\n",
            "    Found existing installation: parso 0.8.4\n",
            "    Uninstalling parso-0.8.4:\n",
            "      Successfully uninstalled parso-0.8.4\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.1\n",
            "    Uninstalling packaging-24.1:\n",
            "      Successfully uninstalled packaging-24.1\n",
            "  Attempting uninstall: openpyxl\n",
            "    Found existing installation: openpyxl 3.1.5\n",
            "    Uninstalling openpyxl-3.1.5:\n",
            "      Successfully uninstalled openpyxl-3.1.5\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 3.3\n",
            "    Uninstalling networkx-3.3:\n",
            "      Successfully uninstalled networkx-3.3\n",
            "  Attempting uninstall: nest-asyncio\n",
            "    Found existing installation: nest-asyncio 1.6.0\n",
            "    Uninstalling nest-asyncio-1.6.0:\n",
            "      Successfully uninstalled nest-asyncio-1.6.0\n",
            "  Attempting uninstall: MarkupSafe\n",
            "    Found existing installation: MarkupSafe 2.1.5\n",
            "    Uninstalling MarkupSafe-2.1.5:\n",
            "      Successfully uninstalled MarkupSafe-2.1.5\n",
            "  Attempting uninstall: joblib\n",
            "    Found existing installation: joblib 1.4.2\n",
            "    Uninstalling joblib-1.4.2:\n",
            "      Successfully uninstalled joblib-1.4.2\n",
            "  Attempting uninstall: importlib-resources\n",
            "    Found existing installation: importlib_resources 6.4.0\n",
            "    Uninstalling importlib_resources-6.4.0:\n",
            "      Successfully uninstalled importlib_resources-6.4.0\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.7\n",
            "    Uninstalling idna-3.7:\n",
            "      Successfully uninstalled idna-3.7\n",
            "  Attempting uninstall: fonttools\n",
            "    Found existing installation: fonttools 4.53.1\n",
            "    Uninstalling fonttools-4.53.1:\n",
            "      Successfully uninstalled fonttools-4.53.1\n",
            "  Attempting uninstall: filelock\n",
            "    Found existing installation: filelock 3.15.4\n",
            "    Uninstalling filelock-3.15.4:\n",
            "      Successfully uninstalled filelock-3.15.4\n",
            "  Attempting uninstall: contourpy\n",
            "    Found existing installation: contourpy 1.2.1\n",
            "    Uninstalling contourpy-1.2.1:\n",
            "      Successfully uninstalled contourpy-1.2.1\n",
            "  Attempting uninstall: charset-normalizer\n",
            "    Found existing installation: charset-normalizer 3.3.2\n",
            "    Uninstalling charset-normalizer-3.3.2:\n",
            "      Successfully uninstalled charset-normalizer-3.3.2\n",
            "  Attempting uninstall: jupyter_core\n",
            "    Found existing installation: jupyter_core 5.7.2\n",
            "    Uninstalling jupyter_core-5.7.2:\n",
            "      Successfully uninstalled jupyter_core-5.7.2\n",
            "  Attempting uninstall: Jinja2\n",
            "    Found existing installation: Jinja2 3.1.4\n",
            "    Uninstalling Jinja2-3.1.4:\n",
            "      Successfully uninstalled Jinja2-3.1.4\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib_metadata 8.1.0\n",
            "    Uninstalling importlib_metadata-8.1.0:\n",
            "      Successfully uninstalled importlib_metadata-8.1.0\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 4.3.3\n",
            "    Uninstalling gensim-4.3.3:\n",
            "      Successfully uninstalled gensim-4.3.3\n",
            "  Attempting uninstall: seaborn\n",
            "    Found existing installation: seaborn 0.13.1\n",
            "    Uninstalling seaborn-0.13.1:\n",
            "      Successfully uninstalled seaborn-0.13.1\n",
            "  Attempting uninstall: pooch\n",
            "    Found existing installation: pooch 1.8.2\n",
            "    Uninstalling pooch-1.8.2:\n",
            "      Successfully uninstalled pooch-1.8.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "xgboost 2.1.0 requires nvidia-nccl-cu12; platform_system == \"Linux\" and platform_machine != \"aarch64\", which is not installed.\n",
            "bokeh 3.4.2 requires contourpy>=1.2, but you have contourpy 1.1.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Jinja2-3.1.2 MarkupSafe-2.1.3 Pygments-2.17.2 SimpleITK-2.3.0 asttokens-2.4.1 bio-1.6.0 biopython-1.81 biothings-client-0.3.1 charset-normalizer-2.1.1 comm-0.2.0 contourpy-1.1.1 executing-2.0.1 filelock-3.9.0 fonttools-4.45.0 gensim-4.3.2 gprofiler-official-1.0.0 idna-3.4 importlib-metadata-6.9.0 importlib-resources-6.1.1 jedi-0.19.1 joblib-1.3.2 jupyter_core-5.5.0 mygene-3.2.2 nest-asyncio-1.5.8 networkx-3.0 openpyxl-3.1.2 packaging-23.2 parso-0.8.3 pickle5-0.0.11 platformdirs-4.0.0 pooch-1.8.0 pure-eval-0.2.2 pyparsing-3.1.1 pytz-2023.3.post1 scipy-1.10.1 seaborn-0.13.0 smart-open-6.4.0 stack-data-0.6.3 sympy-1.12 threadpoolctl-3.2.0 tqdm-4.66.1 traitlets-5.14.0 tzdata-2023.3 urllib3-1.26.13 zipp-3.17.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 0.3 upload file to colab\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "    print('User uploaded file \"{name}\" with length {length} bytes'.format(name=fn, length=len(uploaded[fn])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 40
        },
        "id": "5eFMFwVlna4o",
        "outputId": "304b935b-52e8-4fec-9c39-b06a1a87d1d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-0c60bafd-6be2-4b5c-96ae-6f1a8855f3c6\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-0c60bafd-6be2-4b5c-96ae-6f1a8855f3c6\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.1 Enter a single sequence to be tested, and select a code cell to run as needed(1.1 or 1.2).\n",
        "single = 1\n",
        "single_test_sequence = ['MSGGGVIRGPAGNNDCRIYVGNLPPDIRTKDIEDVFYKYGAIRDIDLKNRRGGPPFAFVEFEDPRDAEDAVYGRDGYDYDGYRLRVEFPRSGRGTGRGGGGGGGGGAPRGRYGPPSRRSENRVVVSGLPPSGSWQDLKDHMREAGDVCYADVYRDGTGVVEFVRKEDMTYAVRKLDNTKFRSHEGETAYIRVKVDGPRSPSYGRSRSRSRSRSRSRSRSNSRSRSYSPRRSRGSPRYSPRHSRSRSRT'] # please enter your protein sequence\n",
        "\n",
        "\n",
        "pos_test_sequence = [protein.lower() for protein in single_test_sequence]\n",
        "neg_test_sequence = [protein.lower() for protein in single_test_sequence]"
      ],
      "metadata": {
        "id": "PFUNOqZLUdTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.2 Enter the file address of the txt file containing the test sequences, one sequence per line, please select a code cell to run as needed(1.1 or 1.2).\n",
        "pos_test_dir = \"pos_word_list_mydata_test.txt\" # Needs to be modified to your test dataset file\n",
        "neg_test_dir = \"\"\n",
        "single = 0\n",
        "def readdata_test(pos_protein_dir, neg_protein_dir):\n",
        "    pos_protein_path = pos_protein_dir\n",
        "    neg_protein_path = neg_protein_dir\n",
        "    if pos_protein_path == '':\n",
        "        pos_word_list = []\n",
        "    else:\n",
        "      with open(pos_protein_path, 'r') as f:\n",
        "          pos_word_list = f.read().splitlines()\n",
        "\n",
        "    if neg_protein_path == '':\n",
        "        neg_word_list = []\n",
        "    else:\n",
        "      with open(neg_protein_path, 'r') as f:\n",
        "          neg_word_list = f.read().splitlines()\n",
        "\n",
        "    pos_sequence = [protein.lower() for protein in pos_word_list]\n",
        "    neg_sequence = [protein.lower() for protein in neg_word_list]\n",
        "    return pos_sequence, neg_sequence\n",
        "\n",
        "pos_test_sequence,neg_test_sequence = readdata_test(pos_test_dir, neg_test_dir)"
      ],
      "metadata": {
        "id": "WhhD8vtJTN4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.Define the model\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data import dataset, dataloader\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from torch import LongTensor, Tensor, from_numpy, max_pool1d, nn, unsqueeze,optim\n",
        "import argparse\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn import metrics\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import pandas as pd\n",
        "import copy\n",
        "import random\n",
        "def readdata(pos_protein_dir, neg_protein_dir, length, pos_seed, neg_seed):\n",
        "    pos_protein_path = pos_protein_dir\n",
        "    neg_protein_path = neg_protein_dir\n",
        "    with open(pos_protein_path, 'r') as f:\n",
        "        pos_word_list = f.read().splitlines()\n",
        "    f.close\n",
        "    with open(neg_protein_path, 'r') as f:\n",
        "        neg_word_list = f.read().splitlines()\n",
        "    f.close\n",
        "\n",
        "    np.random.seed(pos_seed)\n",
        "    np.random.shuffle(pos_word_list)\n",
        "    np.random.seed(neg_seed)\n",
        "    np.random.shuffle(neg_word_list)\n",
        "    if length is not None:\n",
        "        neg_word_list = neg_word_list[:length]\n",
        "        pos_word_list = pos_word_list[:length]\n",
        "    pos_sequence = pos_word_list\n",
        "    neg_sequence = neg_word_list\n",
        "    return pos_sequence, neg_sequence\n",
        "\n",
        "\n",
        "\n",
        "def word2Num(train, test, min=0, max=None, max_features=None):\n",
        "    dic = {}\n",
        "    count = {}\n",
        "    for list0 in train:\n",
        "        list0 = list0.replace(' ', '')\n",
        "        for word in list0:\n",
        "            count[word] = count.get(word, 0) + 1\n",
        "    if min is not None:\n",
        "        count = {word:value for word,value in count.items() if value>min}\n",
        "    if max is not None:\n",
        "        count = {word:value for word,value in count.items() if value<max}\n",
        "    if  max_features is not None:\n",
        "        temp = sorted(count.items(), key=lambda x:x[-1], reverse=True)[:max_features]\n",
        "        count = dict(temp)\n",
        "    for word in count:\n",
        "        dic[word] = len(dic) + 1\n",
        "    print(dic)\n",
        "    Num = []\n",
        "    for list0 in train:\n",
        "        list0 = list0.replace(' ', '')\n",
        "        num = []\n",
        "        for word in list0:\n",
        "            num.append(dic.get(word))\n",
        "        Num.append(num)\n",
        "    print(len(Num))\n",
        "    Num2 = []\n",
        "    for list0 in test:\n",
        "        list0 = list0.replace(' ', '')\n",
        "        num2 = []\n",
        "        for word in list0:\n",
        "            num2.append(dic.get(word))\n",
        "        Num2.append(num2)\n",
        "    print(len(Num2))\n",
        "    return Num, Num2, dic\n",
        "\n",
        "\n",
        "def collate_fn(data):\n",
        "    data.sort(key=lambda tuple: len(tuple[0]), reverse=True)\n",
        "    data_length = [len(tuple[0]) for tuple in data]\n",
        "    data_ten, data_label = [], []\n",
        "    for tuple in data:\n",
        "        data_ten.append(tuple[0])\n",
        "        data_label.append(tuple[1])\n",
        "    data_ten = pad_sequence(data_ten, batch_first=True,padding_value=0)\n",
        "    data_label = torch.LongTensor(data_label)\n",
        "    data_length = torch.LongTensor(data_length)\n",
        "    return data_ten, data_label, data_length\n",
        "\n",
        "\n",
        "class Mydata(dataset.Dataset):\n",
        "    def __init__(self, data, label):\n",
        "        self.data = data\n",
        "        self.label = label\n",
        "    def __getitem__(self, idx):\n",
        "        protein = self.data[idx]\n",
        "        label = self.label[idx]\n",
        "        return protein, label\n",
        "    def __len__(self):\n",
        "        assert len(self.data)==len(self.label)\n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "class ECALayer(nn.Module):\n",
        "    \"\"\"Constructs a ECA module.\n",
        "    Args:\n",
        "        channel: Number of channels of the input feature map\n",
        "        k_size: Adaptive selection of kernel size\n",
        "    \"\"\"\n",
        "    def __init__(self, k_size=5): # 3 5\n",
        "        super(ECALayer, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.conv = nn.Conv1d(1, 1, kernel_size=k_size, padding=(k_size - 1) // 2, bias=False)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x, length):\n",
        "        # feature descriptor on the global spatial information\n",
        "        b, e, t = x.size()\n",
        "\n",
        "        for i in range(b):\n",
        "            x_pack = x[i][: , : length[i]].unsqueeze(0)\n",
        "            x_avg = self.avg_pool(x_pack)\n",
        "            if i == 0:\n",
        "                y = x_avg.clone()\n",
        "            else:\n",
        "                y = torch.cat((y, x_avg), dim=0)\n",
        "\n",
        "        # Two different branches of ECA module\n",
        "        y = self.conv(y.transpose(-1, -2)).transpose(-1, -2)\n",
        "\n",
        "        # Multi-scale information fusion\n",
        "        y = self.sigmoid(y)\n",
        "\n",
        "        return x * y.expand_as(x)\n",
        "\n",
        "\n",
        "class RCNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_num, hidden_dim, num_layers, biFlag, dropout=0.2):\n",
        "        super(RCNN, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_num = embedding_num\n",
        "        self.hiddern_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        if biFlag:\n",
        "            self.bi_num = 2\n",
        "        else:\n",
        "            self.bi_num = 1\n",
        "        self.biFlag = biFlag\n",
        "        self.device = torch.device(\"cuda\")\n",
        "        self.ECABlock= ECALayer()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_num, padding_idx=0)   # 需要添加padding_idx\n",
        "        self.lstm = nn.LSTM(input_size= embedding_num, hidden_size=hidden_dim, num_layers=num_layers, batch_first=True, bidirectional=biFlag)\n",
        "        self.globalmaxpool = nn.AdaptiveMaxPool1d(1)\n",
        "        self.linear = nn.Sequential(\n",
        "            nn.Dropout(dropout),\n",
        "\n",
        "            nn.Linear(self.bi_num*hidden_dim + embedding_num, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(128,32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32,2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, length):\n",
        "        embed = self.embedding(x)\n",
        "        x = pack_padded_sequence(embed, length.cpu(), batch_first=True)\n",
        "        x, (ht,ct) = self.lstm(x)\n",
        "        out, out_len = pad_packed_sequence(x, batch_first=True)\n",
        "        out = torch.cat((embed, out), 2)\n",
        "        out = F.relu(out)\n",
        "        out = out.permute(0, 2, 1)\n",
        "\n",
        "        out1 = self.ECABlock(out, length)\n",
        "        out = out + out1\n",
        "\n",
        "        out = self.globalmaxpool(out).squeeze()\n",
        "        out = F.relu(out)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def set_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False"
      ],
      "metadata": {
        "id": "Bjmxy3bGMeXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.running model\n",
        "if __name__== '__main__':\n",
        "    device = torch.device(\"cpu\")\n",
        "    seed = 1\n",
        "    set_seed(seed)\n",
        "    pos_protein_dir = 'pos_word_list_mydata_all_1507.txt'\n",
        "    neg_protein_dir = 'neg_word_list_1479.txt'\n",
        "    model_path = 'model_mydata_1.pt'\n",
        "    list_length = 1479\n",
        "\n",
        "    pos_seed_list = [20]\n",
        "    neg_seed_list = [21]\n",
        "\n",
        "\n",
        "    for i in range(len(pos_seed_list)):\n",
        "        pos_seed = pos_seed_list[i]\n",
        "        neg_seed = neg_seed_list[i]\n",
        "        pos_sequence, neg_sequence = readdata(pos_protein_dir, neg_protein_dir, list_length, pos_seed, neg_seed)\n",
        "\n",
        "        if not os.path.exists(\"classification_output/personal_output\"):\n",
        "            os.makedirs(\"classification_output/personal_output\")\n",
        "        # auc_save_csv = 'classification_output/personal_output/personal_test_roc_{}.csv'.format((i+1))\n",
        "        result_save_csv = 'classification_output/personal_output/result_{}.csv'.format((i+1))\n",
        "        # df_test = pd.DataFrame(columns=['y_true', 'y_score'])\n",
        "        # df_test.to_csv(auc_save_csv, mode='w', index=False)\n",
        "        df_test = pd.DataFrame(columns=['acc', 'sen', 'spe', 'auc'])\n",
        "        df_test.to_csv(result_save_csv, index=False)\n",
        "\n",
        "        neg_num = len(neg_test_sequence)\n",
        "        pos_num = len(pos_test_sequence)\n",
        "        print('pos_num=',pos_num)\n",
        "        print('neg_num=',neg_num)\n",
        "\n",
        "        neg_num = len(neg_sequence)\n",
        "        pos_num = len(pos_sequence)\n",
        "\n",
        "        start = 0\n",
        "        interval = 0.1\n",
        "        val_split = 0.1\n",
        "\n",
        "        total_tp = 0\n",
        "        total_p = 0\n",
        "        total_n = 0\n",
        "        total_tn = 0\n",
        "\n",
        "        save_sen = []\n",
        "        save_spe = []\n",
        "        save_acc = []\n",
        "\n",
        "        fold = 0\n",
        "        total_correct, total_F1, total_R, total_precision = [],[],[],[]\n",
        "\n",
        "\n",
        "        test_pos_seq = pos_test_sequence\n",
        "        test_neg_seq = neg_test_sequence\n",
        "\n",
        "        train_val_pos_seq = pos_sequence[:int(pos_num*start)] + pos_sequence[int(pos_num*(start+interval)):]\n",
        "        train_val_neg_seq = neg_sequence[:int(neg_num * start)] + neg_sequence[int(neg_num * (start + interval)):]\n",
        "        train_val_pos_num = len(train_val_pos_seq)\n",
        "        train_val_neg_num = len(train_val_neg_seq)\n",
        "        np.random.shuffle(train_val_pos_seq)\n",
        "        np.random.shuffle(train_val_neg_seq)\n",
        "        val_pos_seq = train_val_pos_seq[:int(train_val_pos_num*val_split)]\n",
        "        train_pos_seq = train_val_pos_seq[int(train_val_pos_num*val_split):]\n",
        "        val_neg_seq = train_val_neg_seq[:int(train_val_neg_num*val_split)]\n",
        "        train_neg_seq = train_val_neg_seq[int(train_val_neg_num*val_split):]\n",
        "\n",
        "        test_y = np.hstack((np.zeros(shape=(len(test_neg_seq), )),\n",
        "                        np.ones(shape=(len(test_pos_seq), ))))\n",
        "\n",
        "        print('test_pos', test_y[test_y == 1].shape)\n",
        "        print('test_neg', test_y[test_y == 0].shape)\n",
        "\n",
        "        if (len(test_neg_seq) and len(test_pos_seq)):\n",
        "          flag = 1\n",
        "        else:\n",
        "          flag = 0\n",
        "\n",
        "        train_seq = train_neg_seq + train_pos_seq\n",
        "        val_seq = val_neg_seq + val_pos_seq\n",
        "        train_val_seq = train_seq + val_seq\n",
        "        test_seq = test_neg_seq + test_pos_seq\n",
        "\n",
        "        state = np.random.get_state()\n",
        "        np.random.shuffle(test_seq)\n",
        "        np.random.set_state(state)\n",
        "        np.random.shuffle(test_y)\n",
        "\n",
        "        _, test_num, vocab  = word2Num(train_seq, test_seq)\n",
        "        test_data_size = len(test_num)\n",
        "\n",
        "\n",
        "        test_ten = []\n",
        "        for list1 in test_num:\n",
        "            test_ten.append(torch.LongTensor(list1))\n",
        "\n",
        "        test_label_ten = from_numpy(test_y)\n",
        "        test_label_ten = test_label_ten.type(torch.LongTensor)\n",
        "\n",
        "        state_dict = torch.load(model_path, map_location=torch.device('cpu'))\n",
        "        rcnn = RCNN(len(vocab)+1, 1024, 100, 1, True)\n",
        "        rcnn = rcnn.to(device)\n",
        "        rcnn.load_state_dict(state_dict)\n",
        "        rcnn.eval()\n",
        "        print(rcnn)\n",
        "\n",
        "        test = Mydata(test_ten, test_label_ten)\n",
        "\n",
        "        set_seed(seed)\n",
        "        test_dataloader = dataloader.DataLoader(dataset=test, batch_size=32,shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "        test_loss = 0\n",
        "        y_true_test = []\n",
        "        y_pre_test = []\n",
        "        y_score_test = []\n",
        "        total_labels_test = 0\n",
        "        with torch.no_grad():\n",
        "            for input, label, length in test_dataloader:\n",
        "                input = input.to(device)\n",
        "                label = label.to(device)\n",
        "                length = length.to(device)\n",
        "\n",
        "                output = rcnn(input, length)\n",
        "                _, predicted = torch.max(output,1)\n",
        "                y_pre_test.extend(predicted.cpu())\n",
        "                y_true_test.extend(label.cpu())\n",
        "                y_score_test.extend(torch.softmax(output, dim=-1)[:, 1].cpu().detach())\n",
        "                total_labels_test += label.size(0)\n",
        "\n",
        "                test_correct = metrics.accuracy_score(y_true_test, y_pre_test)\n",
        "                test_F1 = metrics.f1_score(y_true_test, y_pre_test, average='macro')\n",
        "                test_R = metrics.recall_score(y_true_test, y_pre_test)\n",
        "                test_precision = metrics.precision_score(y_true_test, y_pre_test)\n",
        "                if flag and (not single):\n",
        "                  test_auc = metrics.roc_auc_score(y_true_test, y_score_test)\n",
        "\n",
        "                  save_content = 'Test: Correct: %.5f, Precision: %.5f, R: %.5f, F1(macro): %.5f, AUC:%.5f, test_loss: %f' % \\\n",
        "                              (test_correct, test_precision, test_R, test_F1, test_auc, test_loss)\n",
        "                  print(save_content)\n",
        "\n",
        "            y_true_data = [i.item() for i in y_true_test]\n",
        "            y_score_data = [i.item() for i in y_score_test]\n",
        "            y_pre_data = [i.item() for i in y_pre_test]\n",
        "            if single:\n",
        "              print({'Seq':test_seq[0], 'y_pre': np.array(y_pre_data)[0], 'y_score':y_score_data[0]})\n",
        "            else:\n",
        "              if not os.path.exists(\"classification_output/personal_output\"):\n",
        "                  os.makedirs(\"classification_output/personal_output\")\n",
        "              auc_save_csv = 'classification_output/personal_output/personal_test_roc_{}.csv'.format((i+1))\n",
        "              df_test = pd.DataFrame(columns=['Seq', 'y_true', 'y_score', 'y_pre'])\n",
        "              df_test.to_csv(auc_save_csv,mode='w', index=False)\n",
        "              auc_dict = {'Seq':test_seq , 'y_true':y_true_data, 'y_pre': np.array(y_pre_data), 'y_score':y_score_data}\n",
        "              auc_score = pd.DataFrame(auc_dict)\n",
        "              auc_score.to_csv(auc_save_csv, mode='a', header=False, index=False, float_format='%.4f')\n",
        "\n",
        "              p = np.array(y_pre_data)[np.array(y_true_data) == 1]\n",
        "              tp = p[p == 1]\n",
        "              n = np.array(y_pre_data)[np.array(y_true_data) == 0]\n",
        "              tn = n[n == 0]\n",
        "\n",
        "              sen = tp.shape[0] / p.shape[0] if p.shape[0] > 0 else 1\n",
        "              spe = tn.shape[0] / n.shape[0] if n.shape[0] > 0 else 1\n",
        "              acc = (tp.shape[0] + tn.shape[0]) / (p.shape[0] + n.shape[0])\n",
        "              if flag:\n",
        "                auc = metrics.roc_auc_score(y_true_data, y_score_data)\n",
        "              else:\n",
        "                auc = 0\n",
        "              print('sen:', sen)\n",
        "              print('spe:', spe)\n",
        "              print('acc:', acc)\n",
        "              print('auc:', auc)\n",
        "\n",
        "              # list1 = [test_loss, test_correct, test_F1, test_R, test_precision, (tn.shape[0] / n.shape[0] if n.shape[0] > 0 else 1), test_auc]\n",
        "              test_dict = {'acc':[acc], 'sen':[sen], 'spe':[spe], 'auc':[auc]}\n",
        "              # list.extend(list1)\n",
        "              # data_test = pd.DataFrame([list])\n",
        "              test_score = pd.DataFrame(test_dict)\n",
        "              print(test_score)\n",
        "              test_score.to_csv(result_save_csv, mode='a', header=False, index=False, float_format='%.4f')"
      ],
      "metadata": {
        "id": "PaScVPYNMMaq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13948d48-3652-43eb-ad7e-f00eb0b750df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pos_num= 147\n",
            "neg_num= 0\n",
            "test_pos (147,)\n",
            "test_neg (0,)\n",
            "{'m': 1, 's': 2, 'w': 3, 'l': 4, 't': 5, 'e': 6, 'd': 7, 'i': 8, 'r': 9, 'g': 10, 'f': 11, 'y': 12, 'k': 13, 'a': 14, 'h': 15, 'p': 16, 'c': 17, 'v': 18, 'n': 19, 'q': 20}\n",
            "2398\n",
            "147\n",
            "RCNN(\n",
            "  (ECABlock): ECALayer(\n",
            "    (avg_pool): AdaptiveAvgPool1d(output_size=1)\n",
            "    (conv): Conv1d(1, 1, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
            "    (sigmoid): Sigmoid()\n",
            "  )\n",
            "  (embedding): Embedding(21, 1024, padding_idx=0)\n",
            "  (lstm): LSTM(1024, 100, batch_first=True, bidirectional=True)\n",
            "  (globalmaxpool): AdaptiveMaxPool1d(output_size=1)\n",
            "  (linear): Sequential(\n",
            "    (0): Dropout(p=0.2, inplace=False)\n",
            "    (1): Linear(in_features=1224, out_features=128, bias=True)\n",
            "    (2): ReLU()\n",
            "    (3): Dropout(p=0.2, inplace=False)\n",
            "    (4): Linear(in_features=128, out_features=32, bias=True)\n",
            "    (5): ReLU()\n",
            "    (6): Linear(in_features=32, out_features=2, bias=True)\n",
            "  )\n",
            ")\n",
            "sen: 0.891156462585034\n",
            "spe: 1\n",
            "acc: 0.891156462585034\n",
            "auc: 0\n",
            "        acc       sen  spe  auc\n",
            "0  0.891156  0.891156    1    0\n"
          ]
        }
      ]
    }
  ]
}